# %%
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import glob
import os
from collections import defaultdict

dronerf_path = r"C:\PRML\DroneRF_Dataset\DroneRF"

def load_rf_signal_fast(file_path, n_samples=50000):
    """Quick load for feature extraction"""
    with open(file_path, 'r') as f:
        data_line = f.readline().strip()
    values = data_line.split(',')[:n_samples]
    return np.array([float(v) for v in values])

def extract_time_domain_features(signal):
    """Extract simple time-domain features for Naive Bayes"""
    return np.array([
        np.sum(signal**2),                          # signal_energy
        np.sqrt(np.mean(signal**2)),                # rms_amplitude
        np.sum(np.diff(np.sign(signal)) != 0) / len(signal),  # zero_crossing_rate
        np.max(signal) - np.min(signal),            # peak_to_peak
        np.mean(np.abs(signal)),                    # mean_abs
        np.std(signal)                              # std
    ])

print("="*70)
print("NAIVE BAYES: BALANCED DATASET PREPARATION")
print("="*70)

# Get all files
csv_files = glob.glob(os.path.join(dronerf_path, '**', '*.csv'), recursive=True)

# Categorize by drone type and mode
drone_categories = defaultdict(list)
background_files = []

for file_path in csv_files:
    if 'background' in file_path.lower():
        background_files.append(file_path)
    else:
        # Extract drone type from path (AR=101, Bebop=100, Phantom=110)
        # and mode from filename pattern
        parent_folder = file_path.split(os.sep)[-2]  # Get parent folder name
        drone_categories[parent_folder].append(file_path)

# Show distribution
print("\nDataset Distribution:")
print(f"Background: {len(background_files)} files")
for category, files in sorted(drone_categories.items()):
    print(f"{category}: {len(files)} files")

# Balanced sampling: take equal samples from each category
samples_per_category = 5  # Adjust based on time/memory constraints
total_drone_categories = len(drone_categories)

sample_files = []
category_labels = []

# Sample from each drone category
for category, files in drone_categories.items():
    sampled = files[:min(samples_per_category, len(files))]
    sample_files.extend(sampled)
    category_labels.extend([category] * len(sampled))

# Sample from background (match total drone samples)
n_background = len(sample_files)
background_sampled = background_files[:min(n_background, len(background_files))]
sample_files.extend(background_sampled)
category_labels.extend(['Background'] * len(background_sampled))

print(f"\nðŸ“Š Balanced Sample:")
print(f"Total files: {len(sample_files)}")
for cat in set(category_labels):
    count = category_labels.count(cat)
    print(f"  {cat}: {count} files")

print("\nExtracting features...")

X_list = []
y_list = []

for i, file_path in enumerate(sample_files):
    if i % 10 == 0:
        print(f"Processing {i+1}/{len(sample_files)}...")
    
    # Load signal
    signal = load_rf_signal_fast(file_path, n_samples=50000)
    
    # Extract features
    features = extract_time_domain_features(signal)
    
    # Label
    label = 1 if 'background' not in file_path.lower() else 0
    
    X_list.append(features)
    y_list.append(label)

X = np.array(X_list)
y = np.array(y_list)

print(f"\nâœ… Feature extraction complete")
print(f"Feature matrix shape: {X.shape}")
# %% Train Logistic Regression Model
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTrain set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

print("="*70)
print("LOGISTIC REGRESSION MODEL")
print("="*70)

# Use the same X_train, X_test, y_train, y_test from Naive Bayes
# Feature scaling (important for Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nTraining Logistic Regression...")

# Train model
lr_model = LogisticRegression(
    max_iter=1000,      # Ensure convergence
    random_state=42,
    class_weight='balanced'  # Handle any class imbalance
)
lr_model.fit(X_train_scaled, y_train)

print("Training complete")
# %%
from sklearn.metrics import precision_score, recall_score, f1_score

y_pred_lr = lr_model.predict(X_test_scaled)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred_lr)
precision = precision_score(y_test, y_pred_lr, average='weighted', zero_division=0)
recall = recall_score(y_test, y_pred_lr, average='weighted')
f1 = f1_score(y_test, y_pred_lr, average='weighted')

# Display results
print("\n" + "="*70)
print("LOGISTIC REGRESSION RESULTS")
print("="*70)
print(f"\nTest Accuracy: {accuracy:.2%}")
print(f"Test Precision: {precision:.2%}")
print(f"Test Recall: {recall:.2%}")
print(f"Test F1-Score: {f1:.2%}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr, target_names=['Background', 'Drone'], zero_division=0))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred_lr)
print(cm)
print(f"\nTrue Negatives: {cm[0,0]}, False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}")

print("\n" + "="*70)
print(f"LOGISTIC REGRESSION: {accuracy:.2%} ACCURACY")
print("="*70)

# %%
# %% MFCC Feature Extraction (OPTIONAL - Running in background)
from scipy import signal as scipy_signal
from scipy.fftpack import dct
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

def extract_mfcc_features(rf_signal, n_mfcc=13, n_fft=512):
    """Extract MFCC features from RF signal"""
    f, t, Sxx = scipy_signal.spectrogram(
        rf_signal, 
        fs=1.0,
        nperseg=n_fft,
        noverlap=n_fft//2
    )
    
    power_spectrum = np.abs(Sxx) + 1e-10
    log_power = np.log(power_spectrum)
    mfcc = dct(log_power, axis=0, norm='ortho')[:n_mfcc]
    
    features = np.concatenate([
        np.mean(mfcc, axis=1),
        np.std(mfcc, axis=1),
        np.max(mfcc, axis=1),
        np.min(mfcc, axis=1)
    ])
    
    return features

print("="*70)
print("EXTRACTING MFCC FEATURES")
print("="*70)

X_mfcc_list = []
y_mfcc_list = []

for i, file_path in enumerate(sample_files):
    if i % 10 == 0:
        print(f"Processing {i+1}/{len(sample_files)}...")
    
    signal_data = load_rf_signal_fast(file_path, n_samples=50000)
    features = extract_mfcc_features(signal_data)
    label = 1 if 'background' not in file_path.lower() else 0
    
    X_mfcc_list.append(features)
    y_mfcc_list.append(label)

X_mfcc = np.array(X_mfcc_list)
y_mfcc = np.array(y_mfcc_list)

X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc = train_test_split(
    X_mfcc, y_mfcc, test_size=0.2, random_state=42, stratify=y_mfcc
)

scaler_mfcc = StandardScaler()
X_train_mfcc_scaled = scaler_mfcc.fit_transform(X_train_mfcc)
X_test_mfcc_scaled = scaler_mfcc.transform(X_test_mfcc)

lr_mfcc = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
lr_mfcc.fit(X_train_mfcc_scaled, y_train_mfcc)

y_pred_mfcc = lr_mfcc.predict(X_test_mfcc_scaled)
accuracy_mfcc = accuracy_score(y_test_mfcc, y_pred_mfcc)

print(f"\n{'='*70}")
print(f"MFCC Results: {accuracy_mfcc:.2%}")
print(f"Time-domain Results: 52.78%")
print(f"Improvement: {(accuracy_mfcc - 0.5278):+.2%}")
print(f"{'='*70}")
# %%
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
precision = precision_score(y_test_mfcc, y_pred_mfcc)
recall = recall_score(y_test_mfcc, y_pred_mfcc)
F1Score = f1_score(y_test_mfcc, y_pred_mfcc)
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"Recall: {F1Score}")
# %%
